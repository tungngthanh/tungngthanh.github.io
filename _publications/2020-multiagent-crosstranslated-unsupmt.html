---
title: "Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation"
excerpt: "A novel strategy to improve unsupervised MT by using back-translation with multiple models."
collection: publications
date: 2020-03-20
venue: arXiv preprint
paperurl: 'https://arxiv.org/abs/2006.02163'
thumbnail: /images/publications/multiagent-crosstraslated-unsupmt.png
citation: 'Xuan-Phi Nguyen, Shafiq Joty,Thanh-Tung Nguyen, Wu Kui, & Ai Ti Aw (2020). Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation arXiv preprint arXiv:2006.02163.'
---

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<h2>Abstract</h2>
<p>
    Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization,
    language modeling and iterative back-translation, though they may apply these principles differently. This work
    introduces another component to this framework: Multi-Agent Cross-translated Diversification (MACD). The method
    trains multiple UMT agents and then translates monolingual data back and forth using non-duplicative agents to
    acquire synthetic parallel data for supervised MT. MACD is applicable to all previous UMT approaches. In our
    experiments, the technique boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular,
    in WMT'14 English-French, WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked language
    model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements in IWSLT
    English-French and English-German translation tasks. Through extensive experimental analyses, we show that MACD is
    effective because it embraces data diversity while other similar variants do not.
</p>

<h2>Summary</h2>

<p>
    Recent state-of-the-art unsupervised machine translation (UMT) systems usually employ three main principles: initialization,
    language modeling and iterative back-translation, though they may apply these principles differently.
    This paper introduces a new add-on to the three-principle UMT framework:  Multi-Agent Cross-Translated Diversification
    (MACD). The technique is applicable to any existing UMT methods and is shown in this paper to significantly improve
    the performance of the underlying UMT method.
</p>

<p>
    As simple as it may sound, The procedure first trains multiple UMT agents (models), then each of them translates
    the monolingual data from one language $X$ to another $Y$ in the first level. After that, in the second level, the
    generated data are again translated from language $Y$ back to $X$ by agents other than the one of its origin to
    produce level 2 synthetic data - a process which we call <b><i>cross-translation</i></b>. The process continues until
    the $k$-th level. In the final step, a supervised MT model is trained on all those synthetic parallel data.
</p>

<h3>Multi-agent cross-translated diversification (MACD)</h3>
<p>
    Let $S$ and $T$ be the sets of monolingual data for languages $s$ and $t$, respectively. We first train $n$ UMT
    agents $M_1,M_2,...,M_n$, where $n$ is called the <i>diversification multiplier</i>. The agents can be implemented
    following any of the existing UMT techniques. They are trained independently and randomly such that no two agents
    are the same or guaranteed to generate the same output for any given input.
    Since the models are bidirectional, we denote $M_i^{s\rightarrow t}(X_s)=Y$ to be the translations from language
    $s$ to $t$ of corpus $X_s$ (of language $s$) by UMT agent $M_i$, and vice versa for $M_i^{t\rightarrow s}(X_t)=Y$.
    <br>
    The MACD procedure is then summarized in the following algorithm.
</p>

<img src='/images/publications/multi-agent-cross-translated-umt/algorithm1.png'>


<h3>Experiments</h3>

<p>
    We compare our method with common baselines: <a href="https://arxiv.org/abs/1804.07755" target="_blank">NMT & PBSMT</a>
    and <a href="https://arxiv.org/abs/1901.07291" target="_blank">Pretrained XLM</a>. However, we were unable to replicate
    the exact enormous scale setup in those papers, which use the News Crawl 2007-2017 datasets. Instead, we reproduce
    their results using the smaller News Crawl 2007-2008 datasets.
    <br>
    The WMT results are shown as follow. Our method generally improve the performance of the underlying baseline by 2-3 BLEU.
</p>

<img src='/images/publications/multi-agent-cross-translated-umt/experiment-wmt.png'>

<p>
    For IWSLT datasets, we test different variants of MACD against the baseline in the IWSLT'14 De-En and IWSLT'13 En-Fr tasks.
</p>

<img src='/images/publications/multi-agent-cross-translated-umt/experiment-iwslt.png'>

<h3>Understanding MACD</h3>
<p>
    To understand MACD, we run a series of analysis experiments and found that <b>cross-translation</b> plays a vital
    role in the improvement of MACD. Indeed, we tried different variants of methods that use multiple agents without
    cross-translation (MADs) and show that they does not improve the baseline performance.
</p>
<img src='/images/publications/multi-agent-cross-translated-umt/experiment-cross-vs-nocross.png'>

<p>
    Further analyses show that MACD produces hightly diverse data compared to other variants. We compute the reconstruction
    BLEU scores of the back-translation process of MACD and MAD and observe that the score for MACD is much lower than
    MAD's, indicating that the generated data is highly distinct from the originals. Plus, Across the language pairs,
    only around 14% of the parallel data are duplicates. Given that 87.5% of the data are not real, this amount of
    duplicates is surprisingly low.
</p>
<img src='/images/publications/multi-agent-cross-translated-umt/experiment-reconstruct-bleu.png'>
<img src='/images/publications/multi-agent-cross-translated-umt/experiment-duplicates.png'>

<p>
    We also compare MACD with ensemble of models and found that MACD outperforms different ensembling approaches,
    including <a href="https://arxiv.org/abs/1702.01802" target="_blank">Ensemble knowledge distillation</a>.
</p>
<img src='/images/publications/multi-agent-cross-translated-umt/experiment-ensemble.png'>






